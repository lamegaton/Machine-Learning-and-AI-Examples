{
  "metadata": {
    "language_info": {
      "name": ""
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "import numpy as np\n\n# Load the data and labels from the CSV files\ndata = np.loadtxt('train_data.csv', delimiter=',')\nlabels = np.loadtxt('train_labels.csv', delimiter=',')\nnp.shape(data)\n\n# order sample randomly\norder = list(range(np.shape(data)[0]))\nnp.random.shuffle(order)\ntrain = data[order, :]\ntraint = labels[order, :]\n\n\n# This code is inspired from Stephen Marsland\nclass MLP:\n    \"\"\" A Multi-Layer Perceptron\"\"\"\n\n    def __init__(self, inputs, targets, number_of_nodes_in_hidden_layer):\n        \"\"\" Constructor \"\"\"\n        # Set up network size\n        self.nIn = np.shape(inputs)[1]  # return col: number of nodes in Input layer\n        self.nOut = np.shape(targets)[1]  # return col: number of nodes in Output layer\n        self.nHidden = number_of_nodes_in_hidden_layer  # number of nodes in Hidden layer\n\n        # Initialise random scaled weights to weights 1 & weights 2\n        self.weights1 = (np.random.rand(self.nIn + 1, self.nHidden) - 0.5) * 2 / np.sqrt(self.nIn)\n        self.weights2 = (np.random.rand(self.nHidden + 1, self.nOut) - 0.5) * 2 / np.sqrt(self.nHidden)\n\n    def train(self, inputs, targets, eta, n_iterations):\n        # eta = learning rate\n        # Add bias of -1 array into input\n        inputs = np.concatenate((inputs, -np.ones((np.shape(inputs)[0], 1))), axis=1)\n\n        for n in range(n_iterations):\n            # Run Forward\n            # inputs(x) - weights1 -> (self.hidden) hidden (self.hidden + bias) -> weights2 -> output\n            outputs = self.run_fwd(inputs)\n\n            # Start Backpropagation\n            # Cross-Entropy Loss Function\n            cost = -np.mean(targets * np.log(outputs + 1e-8))\n\n            # Error function 1/2*sum(t - o)^2\n            error = 0.5 * np.sum((targets - outputs) ** 2)\n\n            # Find accuracy of the output\n            accuracy = self.check_accuracy(outputs, targets)\n\n            # Print results at end of each iteration\n            print(\"\\nITERATION \", n, \"\\nACCURACY = \", accuracy)\n\n            # Print overall results at end of nth iteration\n            if n == n_iterations - 1:\n                print(\"\\n\\nOVERALL RESULTS: \\nMAX ACCURACY OBTAINED: \", accuracy, \"%\")\n                print(\"\\nMAX ACCURACY OUTPUT: \", self.convert_to_one_hot_output(outputs))\n\n            # error signal output = o(1-o)(t-o)\n            delta_out = outputs * (1.0 - outputs) * (targets - outputs)\n\n            # w_new = w_old + eta*delta*o  # updating new weights to weights 2\n            self.weights2 += eta * (np.dot(np.transpose(self.hidden_sigmoid), delta_out))\n\n            # o(1-o)w*deltao\n            delta_hidden = self.hidden_sigmoid * (1.0 - self.hidden_sigmoid) * (\n                np.dot(delta_out, np.transpose(self.weights2)))\n\n            # updating new weights to weights 1\n            self.weights1 += eta * (np.dot(np.transpose(inputs), delta_hidden[:, :-1]))\n\n    def run_fwd(self, inputs):\n        \"\"\" Run the network forward \"\"\"\n        # inputs(x) - weights1 -> (self.hidden) hidden (self.hidden + bias) -> weights2 -> output\n        # x.w\n        self.hidden = np.dot(inputs, (self.weights1))\n\n        # Run through activation function 1/(1 + e^-(B*x.w))\n        # o1,o2,on ...\n        self.hidden_sigmoid = self.sigmoid(self.hidden)\n\n        # add bias nodes (column)\n        self.hidden_sigmoid = np.concatenate((self.hidden_sigmoid, -np.ones((np.shape(inputs)[0], 1))), axis=1)\n\n        # o = o_h . w\n        outputs = np.dot(self.hidden_sigmoid, self.weights2)\n\n        # Final output after activation function\n        output_sigmoid = self.sigmoid(outputs)\n\n        return output_sigmoid\n\n    def check_accuracy(self, calculated_outputs, targets):\n        predicted_classes = np.argmax(calculated_outputs, axis=1)\n        # Number of classes\n        num_classes = calculated_outputs.shape[1]\n        # Create one-hot encoded representation\n        one_hot = np.zeros((calculated_outputs.shape[0], num_classes))\n        one_hot[np.arange(calculated_outputs.shape[0]), predicted_classes] = 1\n        correctness = np.sum(np.equal(one_hot, targets)) / np.size(one_hot)\n        return correctness * 100\n\n    def convert_to_one_hot_output(self, calculated_outputs):\n        predicted_classes = np.argmax(calculated_outputs, axis=1)\n        # Number of classes\n        num_classes = calculated_outputs.shape[1]\n        # Create one-hot encoded representation\n        one_hot = np.zeros((calculated_outputs.shape[0], num_classes))\n        one_hot[np.arange(calculated_outputs.shape[0]), predicted_classes] = 1\n        return one_hot\n\n    def sigmoid(self, s):\n        return 1 / (1 + np.exp(-s))\n\n\n# Single Function to use the network to predict the test set\ndef train_and_predict(test_data):\n    # Train the MLP\n    net = MLP(train, traint, 60)  # MLP(inputs, targets, number_of_nodes_in_hidden_layer)\n    net.train(train, traint, 0.0001, 50)  # train(inputs, targets, eta, n_iterations)\n\n    # Predict the test set\n    test_data_with_bias = np.concatenate((test_data, -np.ones((np.shape(test_data)[0], 1))), axis=1)\n    predicted_outputs = net.run_fwd(test_data_with_bias)\n    predicted_classes = np.argmax(predicted_outputs, axis=1)\n\n    # Convert predicted classes to one-hot encoded representation\n    num_classes = predicted_outputs.shape[1]\n    one_hot_predictions = np.zeros((predicted_outputs.shape[0], num_classes))\n    one_hot_predictions[np.arange(predicted_outputs.shape[0]), predicted_classes] = 1\n    return one_hot_predictions\n\n\ntrain_and_predict(train)\n",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}