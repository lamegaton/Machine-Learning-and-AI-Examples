{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine pre-processing and pos-processing from YOLOv8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "fE4lz8eZ2OHK"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load the TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_path=f'assets/best_float16.tflite')\n",
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "The below code is extract from ultralitics repo:\n",
    "- LetterBox keep the ratio and add padding to an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "COEzZBtG2j5b"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "class LetterBox:\n",
    "    \"\"\"Resize image and padding for detection, instance segmentation, pose.\"\"\"\n",
    "\n",
    "    def __init__(self, new_shape=(640, 640), auto=False, scaleFill=False, scaleup=True, stride=32):\n",
    "        \"\"\"Initialize LetterBox object with specific parameters.\"\"\"\n",
    "        self.new_shape = new_shape\n",
    "        self.auto = auto\n",
    "        self.scaleFill = scaleFill\n",
    "        self.scaleup = scaleup\n",
    "        self.stride = stride\n",
    "\n",
    "    def __call__(self, labels=None, image=None):\n",
    "        \"\"\"Return updated labels and image with added border.\"\"\"\n",
    "        if labels is None:\n",
    "            labels = {}\n",
    "        img = labels.get('img') if image is None else image\n",
    "        shape = img.shape[:2]  # current shape [height, width]\n",
    "        new_shape = labels.pop('rect_shape', self.new_shape)\n",
    "        if isinstance(new_shape, int):\n",
    "            new_shape = (new_shape, new_shape)\n",
    "\n",
    "        # Scale ratio (new / old)\n",
    "        r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
    "        if not self.scaleup:  # only scale down, do not scale up (for better val mAP)\n",
    "            r = min(r, 1.0)\n",
    "\n",
    "        # Compute padding\n",
    "        ratio = r, r  # width, height ratios\n",
    "        new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
    "        dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n",
    "        if self.auto:  # minimum rectangle\n",
    "            dw, dh = np.mod(dw, self.stride), np.mod(dh, self.stride)  # wh padding\n",
    "        elif self.scaleFill:  # stretch\n",
    "            dw, dh = 0.0, 0.0\n",
    "            new_unpad = (new_shape[1], new_shape[0])\n",
    "            ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios\n",
    "\n",
    "        dw /= 2  # divide padding into 2 sides\n",
    "        dh /= 2\n",
    "        if labels.get('ratio_pad'):\n",
    "            labels['ratio_pad'] = (labels['ratio_pad'], (dw, dh))  # for evaluation\n",
    "\n",
    "        if shape[::-1] != new_unpad:  # resize\n",
    "            img = cv2.resize(img, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
    "        top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
    "        left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
    "        img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT,\n",
    "                                 value=(114, 114, 114))  # add border\n",
    "\n",
    "        if len(labels):\n",
    "            labels = self._update_labels(labels, ratio, dw, dh)\n",
    "            labels['img'] = img\n",
    "            labels['resized_shape'] = new_shape\n",
    "            return labels\n",
    "        else:\n",
    "            return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "qGSA-8Hh2oM1"
   },
   "outputs": [],
   "source": [
    "image_path = \"assets/pipe3.jpg\"\n",
    "img_size = 416\n",
    "# scale image\n",
    "im = [LetterBox(img_size, auto=False, stride=32)(image=cv2.imread(image_path))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![pipe_1](assets/pipe.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow(\"bb\",im[0])\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GoAjgtp02teK",
    "outputId": "19186d27-d7f8-4363-a2c7-c6ecaf49ae33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 416, 416, 3)\n",
      "(1, 416, 416, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "im = np.stack(im)\n",
    "print(im.shape)\n",
    "im = im[..., ::-1].transpose((0, 1, 2, 3))  # BGR to RGB, BHWC to BCHW, (n, 3, h, w)\n",
    "print(im.shape)\n",
    "im = np.ascontiguousarray(im)  # contiguous\n",
    "im = im.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "u5pmFbL92vU3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[0.99215686 0.99215686 0.99215686]\n",
      "   [0.99215686 0.99215686 0.99215686]\n",
      "   [0.99215686 0.99215686 0.99215686]\n",
      "   ...\n",
      "   [0.84705883 0.84313726 0.8235294 ]\n",
      "   [0.84313726 0.827451   0.8117647 ]\n",
      "   [0.8392157  0.81960785 0.80784315]]\n",
      "\n",
      "  [[0.99215686 0.99215686 0.99215686]\n",
      "   [0.99215686 0.99215686 0.99215686]\n",
      "   [0.99215686 0.99215686 0.99215686]\n",
      "   ...\n",
      "   [0.84705883 0.8392157  0.8235294 ]\n",
      "   [0.8392157  0.8235294  0.80784315]\n",
      "   [0.8352941  0.8156863  0.8039216 ]]\n",
      "\n",
      "  [[0.99215686 0.99215686 0.99215686]\n",
      "   [0.99215686 0.99215686 0.99215686]\n",
      "   [0.99215686 0.99215686 0.99215686]\n",
      "   ...\n",
      "   [0.84313726 0.8352941  0.8156863 ]\n",
      "   [0.83137256 0.8156863  0.8       ]\n",
      "   [0.8235294  0.8039216  0.7921569 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.99215686 0.99215686 0.99215686]\n",
      "   [0.99215686 0.99215686 0.99215686]\n",
      "   [0.99215686 0.99215686 0.99215686]\n",
      "   ...\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]]\n",
      "\n",
      "  [[0.99215686 0.99215686 0.99215686]\n",
      "   [0.99215686 0.99215686 0.99215686]\n",
      "   [0.99215686 0.99215686 0.99215686]\n",
      "   ...\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]]\n",
      "\n",
      "  [[0.99215686 0.99215686 0.99215686]\n",
      "   [0.99215686 0.99215686 0.99215686]\n",
      "   [0.99215686 0.99215686 0.99215686]\n",
      "   ...\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]]]]\n"
     ]
    }
   ],
   "source": [
    "im /= 255\n",
    "print(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "CcNwNTtF2wvz"
   },
   "outputs": [],
   "source": [
    "# Step 3: Allocate input and output tensors\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Input details ==\n",
      "name: inputs_0\n",
      "shape: [  1 416 416   3]\n",
      "type: <class 'numpy.float32'>\n",
      "\n",
      "== Output details ==\n",
      "name: Identity\n",
      "shape: [   1    5 3549]\n",
      "type: <class 'numpy.float32'>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"== Input details ==\")\n",
    "print(\"name:\", input_details[0]['name'])\n",
    "print(\"shape:\", input_details[0]['shape'])\n",
    "print(\"type:\", input_details[0]['dtype'])\n",
    "\n",
    "print(\"\\n== Output details ==\")\n",
    "print(\"name:\", output_details[0]['name'])\n",
    "print(\"shape:\", output_details[0]['shape'])\n",
    "print(\"type:\", output_details[0]['dtype'])\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "mUwS8feJ2yDJ"
   },
   "outputs": [],
   "source": [
    "# Step 4: Prepare the input tensor\n",
    "input_data = im\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "OGMZpWar2zjK"
   },
   "outputs": [],
   "source": [
    "# Step 5: Run inference\n",
    "interpreter.invoke()\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-bY-J-tQ21SO",
    "outputId": "dbfbdedb-a518-48ae-b1ea-c0d0bf390dd8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 5, 3549)\n"
     ]
    }
   ],
   "source": [
    "# Process the output_data as needed\n",
    "print(output_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "VOCztgCJ22ho"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nc 1\n",
      "mi 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 3549, 5)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "nc = 0 # number of class\n",
    "conf_thres = 0.25\n",
    "\n",
    "bs = output_data.shape[0]  # batch size\n",
    "nc = nc or (output_data.shape[1] - 4)  # number of classes\n",
    "print(f'nc {nc}')\n",
    "nm = output_data.shape[1] - nc - 4\n",
    "mi = 4 + nc  # mask start index\n",
    "print(f'mi {mi}')\n",
    "xc = np.amax(output_data[:, 4:mi], 1) > conf_thres  # candidates\n",
    "\n",
    "multi_label=False\n",
    "multi_label &= nc > 1  # multiple labels per box (adds 0.5ms/img)\n",
    "\n",
    "prediction = np.transpose(output_data, (0, -1, -2))\n",
    "\n",
    "prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "8Wg72Ki828kF"
   },
   "outputs": [],
   "source": [
    "def xywh2xyxy(x):\n",
    "    \"\"\"\n",
    "    Convert bounding box coordinates from (x, y, width, height) format to (x1, y1, x2, y2) format where (x1, y1) is the\n",
    "    top-left corner and (x2, y2) is the bottom-right corner.\n",
    "\n",
    "    Args:\n",
    "        x (np.ndarray | torch.Tensor): The input bounding box coordinates in (x, y, width, height) format.\n",
    "    Returns:\n",
    "        y (np.ndarray | torch.Tensor): The bounding box coordinates in (x1, y1, x2, y2) format.\n",
    "    \"\"\"\n",
    "    y = np.copy(x)\n",
    "    y[..., 0] = x[..., 0] - x[..., 2] / 2  # top left x\n",
    "    y[..., 1] = x[..., 1] - x[..., 3] / 2  # top left y\n",
    "    y[..., 2] = x[..., 0] + x[..., 2] / 2  # bottom right x\n",
    "    y[..., 3] = x[..., 1] + x[..., 3] / 2  # bottom right y\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "5WSZtcCR2-BE"
   },
   "outputs": [],
   "source": [
    "prediction[..., :4] = xywh2xyxy(prediction[..., :4])  # xywh to xyxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "Uaan8MND2_hz"
   },
   "outputs": [],
   "source": [
    "output = [np.zeros((0, 6 + nm))] * bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1, [array([], shape=(0, 6), dtype=float64)]\n"
     ]
    }
   ],
   "source": [
    "print(f'{len(output)}, {output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POST PROCESSING STEPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "bnjy3Dnq3BBP"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "max_nms=30000\n",
    "agnostic=False\n",
    "max_wh=7680\n",
    "iou_thres = 0.45\n",
    "max_det = 300\n",
    "\n",
    "for xi, x in enumerate(prediction):  # image index, image inference\n",
    "  x = x[xc[xi]]  # confidence\n",
    "\n",
    "  if not x.shape[0]:\n",
    "    continue\n",
    "\n",
    "  # Detections matrix nx6 (xyxy, conf, cls)\n",
    "  box = x[:, :4]\n",
    "  cls = x[:, 4:4+nc]\n",
    "  mask = x[:, 4+nc:4+nc+nm]\n",
    "\n",
    "  conf = np.max(cls, axis=1, keepdims=True)\n",
    "  j = np.argmax(cls, axis=1, keepdims=True)\n",
    "\n",
    "  # Concatenate the arrays along axis 1\n",
    "  x = np.concatenate((box, conf, j.astype(float), mask), axis=1)\n",
    "\n",
    "  # Reshape conf to a 1-dimensional array\n",
    "  conf_flat = conf.flatten()\n",
    "\n",
    "  # Filter the resulting array based on the condition conf_flat > conf_thres\n",
    "  filtered_x = x[conf_flat > conf_thres]\n",
    "\n",
    "  n = filtered_x.shape[0]  # number of boxes\n",
    "\n",
    "  if not n:  # no boxes\n",
    "    continue\n",
    "  if n > max_nms:  # excess boxes\n",
    "    # Sort x based on the 5th column in descending order\n",
    "    sorted_indices = np.argsort(x[:, 4])[::-1]\n",
    "\n",
    "    # Select the top max_nms rows based on the sorted indices\n",
    "    x = x[sorted_indices[:max_nms]]\n",
    "\n",
    "  c = x[:, 5:6] * (0 if agnostic else max_wh)\n",
    "  boxes, scores = x[:, :4] + c, x[:, 4]  # boxes (offset by class), scores\n",
    "\n",
    "  # Apply NMS using cv2.dnn.NMSBoxes function\n",
    "  i = cv2.dnn.NMSBoxes(boxes, scores, score_threshold=0.25, nms_threshold=iou_thres)\n",
    "  i = i[:max_det]  # limit detections\n",
    "\n",
    "  output[xi] = x[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.47504956 0.53656602 0.67172056 0.77014041 0.92168653 0.        ]\n",
      " [0.86402118 0.64960629 0.99995911 0.82900232 0.90886736 0.        ]\n",
      " [0.00838937 0.35965824 0.21227707 0.65425563 0.89321238 0.        ]]\n",
      "[array([[0.47504956, 0.53656602, 0.67172056, 0.77014041, 0.92168653,\n",
      "        0.        ],\n",
      "       [0.86402118, 0.64960629, 0.99995911, 0.82900232, 0.90886736,\n",
      "        0.        ],\n",
      "       [0.00838937, 0.35965824, 0.21227707, 0.65425563, 0.89321238,\n",
      "        0.        ]])]\n"
     ]
    }
   ],
   "source": [
    "print(output[0])\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "xqG9gV6p3CYD"
   },
   "outputs": [],
   "source": [
    "def clip_boxes(boxes, shape):\n",
    "    \"\"\"\n",
    "    It takes a list of bounding boxes and a shape (height, width) and clips the bounding boxes to the\n",
    "    shape\n",
    "\n",
    "    Args:\n",
    "      boxes (torch.Tensor): the bounding boxes to clip\n",
    "      shape (tuple): the shape of the image\n",
    "    \"\"\"\n",
    "    boxes[..., [0, 2]] = boxes[..., [0, 2]].clip(0, shape[1])  # x1, x2\n",
    "    boxes[..., [1, 3]] = boxes[..., [1, 3]].clip(0, shape[0])  # y1, y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "N7IkhjZ53D1p"
   },
   "outputs": [],
   "source": [
    "def scale_boxes(img1_shape, boxes, img0_shape, ratio_pad=None, padding=True):\n",
    "    \"\"\"\n",
    "    Rescales bounding boxes (in the format of xyxy) from the shape of the image they were originally specified in\n",
    "    (img1_shape) to the shape of a different image (img0_shape).\n",
    "\n",
    "    Args:\n",
    "      img1_shape (tuple): The shape of the image that the bounding boxes are for, in the format of (height, width).\n",
    "      boxes (torch.Tensor): the bounding boxes of the objects in the image, in the format of (x1, y1, x2, y2)\n",
    "      img0_shape (tuple): the shape of the target image, in the format of (height, width).\n",
    "      ratio_pad (tuple): a tuple of (ratio, pad) for scaling the boxes. If not provided, the ratio and pad will be\n",
    "                         calculated based on the size difference between the two images.\n",
    "      padding (bool): If True, assuming the boxes is based on image augmented by yolo style. If False then do regular\n",
    "        rescaling.\n",
    "\n",
    "    Returns:\n",
    "      boxes (torch.Tensor): The scaled bounding boxes, in the format of (x1, y1, x2, y2)\n",
    "    \"\"\"\n",
    "    if ratio_pad is None:  # calculate from img0_shape\n",
    "        gain = min(img1_shape[0] / img0_shape[0], img1_shape[1] / img0_shape[1])  # gain  = old / new\n",
    "        pad = round((img1_shape[1] - img0_shape[1] * gain) / 2 - 0.1), round(\n",
    "            (img1_shape[0] - img0_shape[0] * gain) / 2 - 0.1)  # wh padding\n",
    "    else:\n",
    "        gain = ratio_pad[0][0]\n",
    "        pad = ratio_pad[1]\n",
    "\n",
    "    if padding:\n",
    "        boxes[..., [0, 2]] -= pad[0]  # x padding\n",
    "        boxes[..., [1, 3]] -= pad[1]  # y padding\n",
    "    boxes[..., :4] /= gain\n",
    "    clip_boxes(boxes, img0_shape)\n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "xCni0Vnj3FKd"
   },
   "outputs": [],
   "source": [
    "# results = []\n",
    "\n",
    "img = cv2.imread(image_path)\n",
    "\n",
    "# for i, pred in enumerate(output):\n",
    "#   pred[:, :4] = scale_boxes((416, 416), pred[:, :4], img.shape)\n",
    "#   results.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u4dDewgP9OAS",
    "outputId": "70920314-a4de-48f1-dcc4-2edd8a65c082",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "225"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "[array([[0.47504956, 0.53656602, 0.67172056, 0.77014041, 0.92168653,\n",
      "        0.        ],\n",
      "       [0.86402118, 0.64960629, 0.99995911, 0.82900232, 0.90886736,\n",
      "        0.        ],\n",
      "       [0.00838937, 0.35965824, 0.21227707, 0.65425563, 0.89321238,\n",
      "        0.        ]])]\n"
     ]
    }
   ],
   "source": [
    "print(len(output[0]))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225\n"
     ]
    }
   ],
   "source": [
    "size = img.shape[0]\n",
    "print(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xP85WiKp3G-a",
    "outputId": "00964edb-c4ce-4317-a748-907717c4bee0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.47504956 0.53656602 0.67172056 0.77014041 0.92168653 0.        ]\n",
      "106 120 151 173\n",
      "[0.86402118 0.64960629 0.99995911 0.82900232 0.90886736 0.        ]\n",
      "194 146 224 186\n",
      "[0.00838937 0.35965824 0.21227707 0.65425563 0.89321238 0.        ]\n",
      "1 80 47 147\n"
     ]
    }
   ],
   "source": [
    "# for detection in results[0]:\n",
    "for detection in output[0]:\n",
    "    print(detection)\n",
    "    xmin, ymin, width, height, conf, class_id = detection\n",
    "    \n",
    "    # Convert float coordinates to integers\n",
    "    xmin = int(xmin*size)\n",
    "    ymin = int(ymin*size)\n",
    "    width = int(width*size)\n",
    "    height = int(height*size)\n",
    "\n",
    "    print(f'{xmin} {ymin} {width} {height}')\n",
    "    \n",
    "    # Draw the rectangle on the image\n",
    "    cv2.rectangle(img, (xmin, ymin), (width, height), (0, 255, 0), 2)\n",
    "\n",
    "    # Add text label\n",
    "    label = f\"{conf:.2f}\"\n",
    "    cv2.putText(img, label, (xmin, ymin - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "eHuFi8ma3Ipq",
    "outputId": "a8f48380-d165-4feb-b92c-85f3b1f4fa18"
   },
   "outputs": [],
   "source": [
    "# Display the image with the rectangle\n",
    "cv2.imshow(\"Image with Bounding Box\", img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![pipe_result](assets/pipe_result.png)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
