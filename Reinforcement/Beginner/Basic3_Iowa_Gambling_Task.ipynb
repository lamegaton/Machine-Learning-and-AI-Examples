{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iowa Gambling Task is an interesting research point out unconscious human decision. In the experiments, each player is given \\$2000. There are four stacks of cards, A, B, C, D. Each of them has positive and negative rewards. After 100 turn, the game will end. Stack A, and B guarantee that player will get negative networth. In constrast, if player draws cards, from C and D. They will get positive networth.\n",
    "\n",
    "Review of SARSA (On-Policy):\n",
    "$$Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left( r + \\gamma Q(s', a') - Q(s, a) \\right)\n",
    "$$\n",
    "\n",
    "- **$Q(s, a)$**: Current Q-value of choosing a particular deck $a$ when the agent has a certain amount of money $s$.\n",
    "- **$r$**: Reward received after choosing the deck (could be positive or negative).\n",
    "- **$Q(s', a')$**: Q-value of the next action $a'$ that will actually be taken in the new state $s'$ according to the current policy.\n",
    "- **$\\alpha$**: Learning rate.\n",
    "- **$\\gamma$**: Discount factor.\n",
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"ql2.svg\" />\n",
    "</div>\n",
    "\n",
    "1. Next step is to define envirnment, state, action, reward, and hyperparameter:\n",
    "\n",
    "    **state**: current money  \n",
    "    **action**: draw card from one of four decks  \n",
    "    **reward**: monetary gain or loss  \n",
    "    **Q-table**: calculate future reward for each state  \n",
    "\n",
    "    **learning rate** $\\alpha$: 0.1  \n",
    "    **discount factor** $\\gamma$: 0.9 (review: discount factor is to reduce the future value)  \n",
    "    **exploration rate** $\\eta$: 0.1  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q_table.shape: [0. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4900,\n",
       " array([119.01536325,  83.33828231,  10.12354408,   0.        ]),\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Initialize parameters\n",
    "alpha = 0.1  # Learning rate\n",
    "gamma = 0.9  # Discount factor\n",
    "epsilon = 0.1  # Exploration rate\n",
    "initial_money = 2000  # Initial money\n",
    "num_turns = 100  # Number of turns\n",
    "num_decks = 4  # Number of decks\n",
    "\n",
    "# Initialize Q-table with zeros\n",
    "Q_table = np.zeros((num_decks,))\n",
    "print(f\"Q_table.shape: {Q_table}\")\n",
    "\n",
    "# Reward structure for each deck\n",
    "def get_reward(deck, money):\n",
    "    if deck == 0:  # Deck A\n",
    "        reward = 100\n",
    "        loss = -150 if random.random() < 0.5 else 0\n",
    "    elif deck == 1:  # Deck B\n",
    "        reward = 200\n",
    "        loss = -250 if random.random() < 0.5 else 0\n",
    "    elif deck == 2:  # Deck C\n",
    "        reward = 50\n",
    "        loss = -25 if random.random() < 0.5 else 0\n",
    "    else:  # Deck D\n",
    "        reward = 25\n",
    "        loss = -75 if random.random() < 0.5 else 0\n",
    "    \n",
    "    # Check if the loss makes the money negative\n",
    "    if money + reward + loss < 0:\n",
    "        return -money  # Lose all the money if it goes negative\n",
    "    else:\n",
    "        return reward + loss\n",
    "\n",
    "# Simulation\n",
    "money = initial_money\n",
    "choices = []\n",
    "\n",
    "for turn in range(num_turns):\n",
    "    # Choose action (deck) based on epsilon-greedy policy\n",
    "    if random.random() < epsilon:\n",
    "        # Exploration: Randomly choose a deck\n",
    "        chosen_deck = random.randint(0, num_decks - 1)\n",
    "    else:\n",
    "        # Exploitation: Choose the deck with the highest Q-value\n",
    "        chosen_deck = np.argmax(Q_table)\n",
    "\n",
    "    # Get reward after choosing the deck\n",
    "    reward = get_reward(chosen_deck, money)\n",
    "    \n",
    "    # Update the money\n",
    "    # if the agent is going to be bankrupt money will equal 0\n",
    "    money += reward\n",
    "    \n",
    "    # Update the Q-value using Q-Learning off-policy method\n",
    "    best_next_action = np.argmax(Q_table)  # Best action for next state\n",
    "    Q_table[chosen_deck] = (1 - alpha) * Q_table[chosen_deck] + alpha * (reward + gamma * Q_table[best_next_action])\n",
    "    \n",
    "    # Record the chosen deck for this turn\n",
    "    choices.append(chosen_deck)\n",
    "\n",
    "# Results\n",
    "final_money = money\n",
    "optimal_choices = choices\n",
    "\n",
    "final_money, Q_table, optimal_choices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upper-Confidence-Bound Action\n",
    "<img src=\"image.png\" height=\"70\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6568.05"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize parameters for training over multiple episodes\n",
    "num_episodes = 1000\n",
    "final_money_list = []\n",
    "optimal_choices_list = []\n",
    "\n",
    "# Run multiple episodes\n",
    "for episode in range(num_episodes):\n",
    "    # Reinitialize Q-table and money for each episode\n",
    "    Q_table = np.zeros((num_decks,))\n",
    "    money = initial_money\n",
    "    choices = []\n",
    "\n",
    "    for turn in range(num_turns):\n",
    "        # Choose action (deck) based on epsilon-greedy policy\n",
    "        if random.random() < epsilon:\n",
    "            # Exploration: Randomly choose a deck\n",
    "            chosen_deck = random.randint(0, num_decks - 1)\n",
    "        else:\n",
    "            # Exploitation: Choose the deck with the highest Q-value\n",
    "            chosen_deck = np.argmax(Q_table)\n",
    "\n",
    "        # Get reward after choosing the deck\n",
    "        reward = get_reward(chosen_deck, money)\n",
    "        \n",
    "        # Update the money\n",
    "        money += reward\n",
    "        \n",
    "        # Update Q-value for the chosen deck\n",
    "        best_next_action = np.argmax(Q_table)  # Best action for next state\n",
    "        Q_table[chosen_deck] = (1 - alpha) * Q_table[chosen_deck] + alpha * (reward + gamma * Q_table[best_next_action])\n",
    "        \n",
    "        # Record the chosen deck for this turn\n",
    "        choices.append(chosen_deck)\n",
    "\n",
    "    # Record results for this episode\n",
    "    final_money_list.append(money)\n",
    "    optimal_choices_list.append(choices)\n",
    "\n",
    "# Calculate average final money over all episodes\n",
    "average_final_money = np.mean(final_money_list)\n",
    "\n",
    "average_final_money\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing between on-policy and off-policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1730.95, 1701.75)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the necessary libraries again and redefining the functions and parameters\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Initialize parameters\n",
    "alpha = 0.1  # Learning rate\n",
    "gamma = 0.9  # Discount factor\n",
    "epsilon = 0.1  # Exploration rate\n",
    "num_decks = 4  # Number of decks in the Iowa Gambling Task\n",
    "num_turns = 100  # Number of turns in each episode\n",
    "num_episodes = 1000  # Number of episodes for training\n",
    "\n",
    "# Function to get reward based on the chosen deck and current money\n",
    "def get_reward(deck, current_money):\n",
    "    if deck == 0:\n",
    "        return np.random.choice([100, -150])\n",
    "    elif deck == 1:\n",
    "        return np.random.choice([200, -250])\n",
    "    elif deck == 2:\n",
    "        return np.random.choice([50, -50])\n",
    "    else:\n",
    "        return np.random.choice([100, -100])\n",
    "\n",
    "# Function to run a single episode using SARSA\n",
    "def run_episode_sarsa(Q_table):\n",
    "    money = 2000  # Initial money\n",
    "    for turn in range(num_turns):\n",
    "        # Choose action (deck) based on epsilon-greedy policy\n",
    "        if np.random.random() < epsilon:\n",
    "            chosen_deck = np.random.randint(0, num_decks)\n",
    "        else:\n",
    "            chosen_deck = np.argmax(Q_table)\n",
    "        \n",
    "        # Get reward after choosing the deck\n",
    "        reward = get_reward(chosen_deck, money)\n",
    "        \n",
    "        # Choose next action (deck) based on epsilon-greedy policy for next state\n",
    "        if np.random.random() < epsilon:\n",
    "            next_chosen_deck = np.random.randint(0, num_decks)\n",
    "        else:\n",
    "            next_chosen_deck = np.argmax(Q_table)\n",
    "        \n",
    "        # Update the Q-value using SARSA\n",
    "        Q_table[chosen_deck] = (1 - alpha) * Q_table[chosen_deck] + \\\n",
    "                                alpha * (reward + gamma * Q_table[next_chosen_deck])\n",
    "        \n",
    "        # Update the money\n",
    "        money += reward\n",
    "    return money\n",
    "\n",
    "# Function to run a single episode using Q-Learning\n",
    "def run_episode_q_learning(Q_table):\n",
    "    money = 2000  # Initial money\n",
    "    for turn in range(num_turns):\n",
    "        # Choose action (deck) based on epsilon-greedy policy\n",
    "        if np.random.random() < epsilon:\n",
    "            chosen_deck = np.random.randint(0, num_decks)\n",
    "        else:\n",
    "            chosen_deck = np.argmax(Q_table)\n",
    "        \n",
    "        # Get reward after choosing the deck\n",
    "        reward = get_reward(chosen_deck, money)\n",
    "        \n",
    "        # Update the Q-value using Q-Learning\n",
    "        Q_table[chosen_deck] = (1 - alpha) * Q_table[chosen_deck] + \\\n",
    "                                alpha * (reward + gamma * np.max(Q_table))\n",
    "        \n",
    "        # Update the money\n",
    "        money += reward\n",
    "    return money\n",
    "\n",
    "# Initialize Q-tables\n",
    "Q_table_sarsa = np.zeros(num_decks)\n",
    "Q_table_q_learning = np.zeros(num_decks)\n",
    "\n",
    "# Run SARSA and Q-Learning for multiple episodes and store final money\n",
    "final_money_sarsa = []\n",
    "final_money_q_learning = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    final_money_sarsa.append(run_episode_sarsa(Q_table_sarsa))\n",
    "    final_money_q_learning.append(run_episode_q_learning(Q_table_q_learning))\n",
    "\n",
    "# Calculate average final money for both methods\n",
    "average_final_money_sarsa = np.mean(final_money_sarsa)\n",
    "average_final_money_q_learning = np.mean(final_money_q_learning)\n",
    "\n",
    "average_final_money_sarsa, average_final_money_q_learning\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
